{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2025 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Getting Started with Google Generative AI using the Gen AI SDK\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.gstatic.com/pantheon/images/bigquery/welcome_page/colab-logo.svg\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fgenerative-ai%2Fmain%2Fgemini%2Fgetting-started%2Fintro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://lh3.googleusercontent.com/JmcxdQi-qOpctIvWKgPtrzZdJJK-J3sWE1RsfjZNwshCFgE_9fULcNpuXYTilIR2hjwN\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/generative-ai/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img src=\"https://www.gstatic.com/images/branding/gcpiconscolors/vertexai/v1/32px.svg\" alt=\"Vertex AI logo\"><br> Open in Vertex AI Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://www.svgrepo.com/download/217753/github.svg\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>\n",
    "\n",
    "\n",
    "<div style=\"clear: both;\"></div>\n",
    "\n",
    "<b>Share to:</b>\n",
    "\n",
    "<a href=\"https://www.linkedin.com/sharing/share-offsite/?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/8/81/LinkedIn_icon.svg\" alt=\"LinkedIn logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://bsky.app/intent/compose?text=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/7/7a/Bluesky_Logo.svg\" alt=\"Bluesky logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://twitter.com/intent/tweet?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/5a/X_icon_2.svg\" alt=\"X logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://reddit.com/submit?url=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://redditinc.com/hubfs/Reddit%20Inc/Brand/Reddit_Logo.png\" alt=\"Reddit logo\">\n",
    "</a>\n",
    "\n",
    "<a href=\"https://www.facebook.com/sharer/sharer.php?u=https%3A//github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/getting-started/intro_genai_sdk.ipynb\" target=\"_blank\">\n",
    "  <img width=\"20px\" src=\"https://upload.wikimedia.org/wikipedia/commons/5/51/Facebook_f_logo_%282019%29.svg\" alt=\"Facebook logo\">\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "84f0f73a0f76"
   },
   "source": [
    "| Author(s) |\n",
    "| --- |\n",
    "| [Eric Dong](https://github.com/gericdong) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvgnzT1CKxrO"
   },
   "source": [
    "## Overview\n",
    "\n",
    "The [Google Gen AI SDK](https://googleapis.github.io/python-genai/) provides a unified interface to Google's generative AI API services. This SDK simplifies the process of integrating generative AI capabilities into applications and services, enabling developers to leverage Google's advanced AI models for various tasks.\n",
    "\n",
    "In this tutorial, you learn about the key features of the Google Gen AI SDK for Python to help you get started with Google generative AI services and models including Gemini. You will complete the following tasks:\n",
    "\n",
    "- Install the Gen AI SDK\n",
    "- Connect to an API service\n",
    "- Send text prompts\n",
    "- Send multimodal prompts\n",
    "- Set system instruction\n",
    "- Configure model parameters\n",
    "- Configure safety filters\n",
    "- Start a multi-turn chat\n",
    "- Control generated output\n",
    "- Generate content stream\n",
    "- Send asynchronous requests\n",
    "- Count tokens and compute tokens\n",
    "- Use context caching\n",
    "- Function calling\n",
    "- Batch prediction\n",
    "- Get text embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No17Cw5hgx12"
   },
   "source": [
    "### Install Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "tFy3H3aPgx12",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet google-genai pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdvJRUWRNGHE"
   },
   "source": [
    "### Use the Google Gen AI SDK\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qgdSpVmDbdQ9"
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "from google import genai\n",
    "from google.genai.types import (\n",
    "    CreateBatchJobConfig,\n",
    "    CreateCachedContentConfig,\n",
    "    EmbedContentConfig,\n",
    "    FunctionDeclaration,\n",
    "    GenerateContentConfig,\n",
    "    HarmBlockThreshold,\n",
    "    HarmCategory,\n",
    "    Part,\n",
    "    SafetySetting,\n",
    "    Tool,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ve4YBlDqzyj9"
   },
   "source": [
    "### Connect to a Generative AI API service\n",
    "\n",
    "Google Gen AI APIs and models including Gemini are available in the following two API services:\n",
    "\n",
    "- **[Google AI for Developers](https://ai.google.dev/gemini-api/docs)**: Experiment, prototype, and deploy small projects.\n",
    "- **[Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs)**: Build enterprise-ready projects on Google Cloud.\n",
    "\n",
    "The Gen AI SDK provided an unified interface to these two API services. This notebook shows how to use the Gen AI SDK in Vertex AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN9kmPKJGAJQ"
   },
   "source": [
    "### Vertex AI\n",
    "\n",
    "To start using Vertex AI, you must have a Google Cloud project and [enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=aiplatform.googleapis.com). Learn more about [setting up a project and a development environment](https://cloud.google.com/vertex-ai/docs/start/cloud-environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DF4l8DTdWgPY"
   },
   "source": [
    "#### Set Google Cloud project information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Nqwi-5ufWp_B",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "PROJECT_ID = \"qwiklabs-gcp-03-b17622e9a8a1\"\n",
    "LOCATION = os.environ.get(\"GOOGLE_CLOUD_REGION\", \"global\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "T-tiytzQE0uM"
   },
   "outputs": [],
   "source": [
    "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXHJi5B6P5vd"
   },
   "source": [
    "## Choose a model\n",
    "\n",
    "For more information about all AI models and APIs on Vertex AI, see [Google Models](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/models#gemini-models) and [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-coEslfWPrxo"
   },
   "outputs": [],
   "source": [
    "MODEL_ID = \"gemini-2.5-flash\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "37CH91ddY9kG"
   },
   "source": [
    "## Send text prompts\n",
    "\n",
    "Use the `generate_content` method to generate responses to your prompts. You can pass text to `generate_content`, and use the `.text` property to get the text content of the response.\n",
    "\n",
    "For more examples of prompt engineering, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/prompts/intro_prompt_design.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6fc324893334"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The largest planet in our solar system is **Jupiter**.\n",
      "\n",
      "It's a gas giant and is so massive that it accounts for more than twice the mass of all the other planets combined.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID, contents=\"What's the largest planet in our solar system?\"\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zurBcEcWhFc6"
   },
   "source": [
    "Optionally, you can display the response in markdown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "3PoF18EwhI7e"
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The largest planet in our solar system is **Jupiter**.\n",
       "\n",
       "It's a gas giant and is so massive that it accounts for more than twice the mass of all the other planets combined."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(response.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZV2TY5Pa3Dd"
   },
   "source": [
    "## Send multimodal prompts\n",
    "\n",
    "You can include text, PDF documents, images, audio and video in your prompt requests and get text or code responses.\n",
    "\n",
    "For more examples of multimodal use cases, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/use-cases/intro_multimodal_use_cases.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "D3SI1X-JVMBj"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Goodbye Lunchtime Stress, Hello Delicious Meal Prep!\n",
      "\n",
      "Staring at this picture, we can almost taste the savory goodness and feel the satisfaction of a healthy, ready-to-eat meal. Who *doesn't* dream of effortlessly delicious, balanced meals waiting for them, whether it's for a busy workday lunch or a quick dinner after a long day? This image perfectly captures the magic of thoughtful meal prep, making healthy eating not just achievable, but truly appealing.\n",
      "\n",
      "Here we see two perfectly portioned containers, each a vibrant symphony of flavors and textures. Tender, glistening chicken pieces, sprinkled with toasted sesame seeds and fresh green onions, sit alongside perfectly steamed broccoli florets. Adding a pop of color and crunch are thinly sliced red bell peppers and carrots, all nestled beside a fluffy bed of rice. It's a classic Asian-inspired combination that promises both comfort and nutrition. The use of clear glass containers not only looks sleek but also speaks to sustainability and easy reheating.\n",
      "\n",
      "This isn't just pretty food; it's smart food. It's the secret weapon against unhealthy takeout, forgotten ingredients, and the dreaded \"what's for dinner?\" dilemma. By taking a little time to prepare meals like these, you're investing in your health, saving money, and gifting yourself the invaluable commodity of time during your busiest moments. So, next time you're planning your week, take a cue from this delicious duo and get prepping! Your future self (and your taste buds) will thank you.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "image = Image.open(\n",
    "    requests.get(\n",
    "        \"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "        stream=True,\n",
    "    ).raw\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        image,\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN6wMdY1RSk3"
   },
   "source": [
    "You can also pass the file URL in `Part.from_uri` in the request to the model directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "pG6l1Fuka6ZJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Your Weekday Lunch Just Got a Delicious Upgrade: The Art of Meal Prep!\n",
      "\n",
      "Who else dreads the \"what's for lunch?\" question mid-week, often leading to rushed, less-than-healthy choices? What if your answer was already waiting, looking *this* good?\n",
      "\n",
      "Just feast your eyes on these vibrant glass containers! This isn't just a pretty picture; it's a testament to the power of meal prepping. Inside, you'll find a perfectly balanced, Asian-inspired feast: fluffy, wholesome rice, tender, glazed chicken (perhaps teriyaki or a similar savory sauce!) generously sprinkled with sesame seeds and chopped green onions. Complementing the protein are crisp, bright green broccoli florets and beautifully julienned carrots and red bell peppers, adding both crunch and a rainbow of nutrients.\n",
      "\n",
      "This setup isn't just about good looks; it's a game-changer for your busy schedule. Imagine grabbing one of these on a hectic Monday morning, knowing you've got a delicious, homemade, and healthy meal ready to go. No more impulse buys, no more sad desk lunches, and definitely no more wondering if you're getting enough veggies.\n",
      "\n",
      "Meal prepping empowers you to:\n",
      "*   **Save time:** Cook once, eat multiple times!\n",
      "*   **Eat healthier:** Control your ingredients and portion sizes.\n",
      "*   **Save money:** Avoid costly takeout and restaurant meals.\n",
      "*   **Reduce stress:** One less decision to make during a busy week.\n",
      "\n",
      "Feeling inspired? It doesn't have to be complicated! Start with one or two simple meals, invest in some good quality, airtight containers like these (glass is great for reheating!), and watch how quickly meal prep transforms your weekdays. Your taste buds and your schedule will thank you!\n",
      "\n",
      "**#MealPrepGoals #HealthyEating #WeekdayMeals #ChickenStirFry #EatWellLiveWell**\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=[\n",
    "        Part.from_uri(\n",
    "            file_uri=\"https://storage.googleapis.com/cloud-samples-data/generative-ai/image/meal.png\",\n",
    "            mime_type=\"image/png\",\n",
    "        ),\n",
    "        \"Write a short and engaging blog post based on this picture.\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "El1lx8P9ElDq"
   },
   "source": [
    "## Set system instruction\n",
    "\n",
    "[System instructions](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/system-instruction-introduction) allow you to steer the behavior of the model. By setting the system instruction, you are giving the model additional context to understand the task, provide more customized responses, and adhere to guidelines over the user interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "7A-yANiyCLaO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "J'aime les bagels.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are a helpful language translator.\n",
    "  Your mission is to translate text in English to French.\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\n",
    "  User input: I like bagels.\n",
    "  Answer:\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIJVEr0RQY8S"
   },
   "source": [
    "## Configure model parameters\n",
    "\n",
    "You can include parameter values in each call that you send to a model to control how the model generates a response. Learn more about [experimenting with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "d9NXP5N2Pmfo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Woof woof! You want a squeaky toy, don't you, little fluffball? Let's talk about the *biggest* toy chest in the whole wide world!\n",
      "\n",
      "Imagine a *GIANT* dog park, bigger than all the parks in the world, filled with *every single squeaky toy imaginable*! Millions and millions of them! That's kind of like the internet!\n",
      "\n",
      "1.  **You Want a Toy! (You want information!)**\n",
      "    You're sitting there, tail wagging, and you suddenly want to know about... squirrels! Or maybe you want to see a video of other puppies playing! So, you give a little bark into your special human-box (that's your computer!).\n",
      "\n",
      "2.  **Your Bark Goes Out! (Your request!)**\n",
      "    That bark, it's like a tiny, urgent *squeak*! It goes zoom! Out of your human-box.\n",
      "\n",
      "3.  **To Your Human's Toy-Thrower! (Your Wi-Fi Router!)**\n",
      "    First, it sniffs its way to your *human's special toy-throwing machine* (that's the Wi-Fi router!). This machine is super smart; it knows all the paths in the big park.\n",
      "\n",
      "4.  **Along the Invisible Leash! (Your Internet Connection/ISP!)**\n",
      "    Your human's machine sends your squeak along a *long, invisible leash* (that's the internet connection, from your ISP!). This leash goes all the way out into the big, big world.\n",
      "\n",
      "5.  **To the Giant Toy Beds! (Servers!)**\n",
      "    This leash goes all the way to a *giant, comfy dog bed* (that's a server!) where millions and millions of squeaky toys are kept. Each toy bed has a special *collar tag* (that's an IP address!) so your squeak knows exactly which bed to go to. And if you barked for 'squeakyball.com' (that's like saying 'the red squeaky ball bed!'), the leash takes your squeak right there!\n",
      "\n",
      "6.  **Finding Your Toy! (The Server finds the data!)**\n",
      "    The big dog bed finds your red squeaky ball! Or the squirrel pictures! Or the puppy video! But it's too big to send all at once. So, it breaks the ball (or the pictures, or the video) into *hundreds of tiny, happy squeaks*!\n",
      "\n",
      "7.  **Squeaks Zoom Back! (Data returns in packets!)**\n",
      "    Zoom! All those tiny squeaks come *racing back* along the invisible leash, past your human's toy-throwing machine, and right back into your human-box!\n",
      "\n",
      "8.  **Your Toy is Whole Again! (Your computer reassembles the data!)**\n",
      "    Your human-box is super smart! It puts all those tiny squeaks back together, perfectly, until... *SQUEAK!* There's your red squeaky ball, right on your screen! Or the squirrel pictures! Or the puppy video!\n",
      "\n",
      "So, the internet is just a super-fast way for your barks (requests) to find squeaky toys (information) in giant dog beds (servers) all over the world, and bring them back to you in tiny, happy squeaks!\n",
      "\n",
      "Now, go chase that tail, little one! Woof!\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me how the internet works, but pretend I'm a puppy who only understands squeaky toys.\",\n",
    "    config=GenerateContentConfig(\n",
    "        temperature=0.4,\n",
    "        top_p=0.95,\n",
    "        top_k=20,\n",
    "        candidate_count=1,\n",
    "        seed=5,\n",
    "        stop_sequences=[\"STOP!\"],\n",
    "        presence_penalty=0.0,\n",
    "        frequency_penalty=0.0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9daipRiUzAY"
   },
   "source": [
    "## Configure safety filters\n",
    "\n",
    "The Gemini API provides safety filters that you can adjust across multiple filter categories to restrict or allow certain types of content. You can use these filters to adjust what's appropriate for your use case. See the [Configure safety filters](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/configure-safety-filters) page for details.\n",
    "\n",
    "For more examples of safety filters, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/responsible-ai/gemini_safety_ratings.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "yPlDRaloU59b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are two disrespectful things you might say to the universe after stubbing your toe in the dark:\n",
      "\n",
      "1.  \"Oh, was that your idea of a cosmic joke, you celestial jerk?\"\n",
      "2.  \"Seriously, Universe? That's the best you could come up with? You're really slipping.\"\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "    Write a list of 2 disrespectful things that I might say to the universe after stubbing my toe in the dark.\n",
    "\"\"\"\n",
    "\n",
    "safety_settings = [\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HARASSMENT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_HATE_SPEECH,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "    SafetySetting(\n",
    "        category=HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,\n",
    "        threshold=HarmBlockThreshold.BLOCK_LOW_AND_ABOVE,\n",
    "    ),\n",
    "]\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        safety_settings=safety_settings,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DpKKhHbx3CaJ"
   },
   "source": [
    "When you make a request to the model, the content is analyzed and assigned a safety rating. You can inspect the safety ratings of the generated content by printing out the model responses, as in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "7R7eyEBetsns"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HATE_SPEECH: 'HARM_CATEGORY_HATE_SPEECH'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=3.43765e-06,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: 'HARM_CATEGORY_DANGEROUS_CONTENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=5.669449e-08,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.0111819655\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_HARASSMENT: 'HARM_CATEGORY_HARASSMENT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=0.00014753682,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>,\n",
      "  severity_score=0.017287662\n",
      "), SafetyRating(\n",
      "  category=<HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: 'HARM_CATEGORY_SEXUALLY_EXPLICIT'>,\n",
      "  probability=<HarmProbability.NEGLIGIBLE: 'NEGLIGIBLE'>,\n",
      "  probability_score=3.0379533e-07,\n",
      "  severity=<HarmSeverity.HARM_SEVERITY_NEGLIGIBLE: 'HARM_SEVERITY_NEGLIGIBLE'>\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(response.candidates[0].safety_ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29jFnHZZWXd7"
   },
   "source": [
    "## Start a multi-turn chat\n",
    "\n",
    "The Gemini API enables you to have freeform conversations across multiple turns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "DbM12JaLWjiF"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert software developer and a helpful coding assistant.\n",
    "  You are able to generate high-quality code in any programming language.\n",
    "\"\"\"\n",
    "\n",
    "chat = client.chats.create(\n",
    "    model=MODEL_ID,\n",
    "    config=GenerateContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        temperature=0.5,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "JQem1halYDBW"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To determine if a year is a leap year, we follow these rules:\n",
      "\n",
      "1.  A year is a leap year if it is **divisible by 4**.\n",
      "2.  However, if it is divisible by **100**, it is **NOT** a leap year.\n",
      "3.  UNLESS it is also divisible by **400**, in which case it **IS** a leap year.\n",
      "\n",
      "This can be summarized with the following logical expression:\n",
      "`(year is divisible by 4 AND year is NOT divisible by 100) OR (year is divisible by 400)`\n",
      "\n",
      "Here's a function implementing this logic in Python:\n",
      "\n",
      "```python\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    Rules for a leap year:\n",
      "    1. It is divisible by 4.\n",
      "    2. EXCEPT if it is divisible by 100.\n",
      "    3. BUT if it is divisible by 400, it IS a leap year.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be a non-negative integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "\n",
      "    Examples:\n",
      "        >>> is_leap_year(2000)  # Divisible by 400\n",
      "        True\n",
      "        >>> is_leap_year(1900)  # Divisible by 100 but not by 400\n",
      "        False\n",
      "        >>> is_leap_year(2004)  # Divisible by 4 but not by 100\n",
      "        True\n",
      "        >>> is_leap_year(2001)  # Not divisible by 4\n",
      "        False\n",
      "        >>> is_leap_year(1600)  # Divisible by 400\n",
      "        True\n",
      "    \"\"\"\n",
      "    # The most concise way to express the leap year rules:\n",
      "    # (divisible by 4 AND not divisible by 100) OR (divisible by 400)\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Examples of Usage ---\n",
      "print(f\"Is 2000 a leap year? {is_leap_year(2000)}\")  # Expected: True (divisible by 400)\n",
      "print(f\"Is 1900 a leap year? {is_leap_year(1900)}\")  # Expected: False (divisible by 100, not 400)\n",
      "print(f\"Is 2004 a leap year? {is_leap_year(2004)}\")  # Expected: True (divisible by 4, not 100)\n",
      "print(f\"Is 2001 a leap year? {is_leap_year(2001)}\")  # Expected: False (not divisible by 4)\n",
      "print(f\"Is 1600 a leap year? {is_leap_year(1600)}\")  # Expected: True (divisible by 400)\n",
      "print(f\"Is 1800 a leap year? {is_leap_year(1800)}\")  # Expected: False (divisible by 100, not 400)\n",
      "print(f\"Is 2024 a leap year? {is_leap_year(2024)}\")  # Expected: True\n",
      "print(f\"Is 2023 a leap year? {is_leap_year(2023)}\")  # Expected: False\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Write a function that checks if a year is a leap year.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "6Fn69TurZ9DB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's a unit test for the `is_leap_year` function using Python's built-in `unittest` module.\n",
      "\n",
      "First, let's ensure we have the `is_leap_year` function available. I'll include it again for completeness, as it would typically be in a separate file (e.g., `date_utils.py`) and imported.\n",
      "\n",
      "```python\n",
      "import unittest\n",
      "\n",
      "# The function to be tested (assuming it's in the same file or imported)\n",
      "def is_leap_year(year: int) -> bool:\n",
      "    \"\"\"\n",
      "    Checks if a given year is a leap year according to the Gregorian calendar rules.\n",
      "\n",
      "    Rules for a leap year:\n",
      "    1. It is divisible by 4.\n",
      "    2. EXCEPT if it is divisible by 100.\n",
      "    3. BUT if it is divisible by 400, it IS a leap year.\n",
      "\n",
      "    Args:\n",
      "        year (int): The year to check. Must be an integer.\n",
      "\n",
      "    Returns:\n",
      "        bool: True if the year is a leap year, False otherwise.\n",
      "    \"\"\"\n",
      "    return (year % 4 == 0 and year % 100 != 0) or (year % 400 == 0)\n",
      "\n",
      "# --- Unit Test Class ---\n",
      "class TestIsLeapYear(unittest.TestCase):\n",
      "    \"\"\"\n",
      "    Unit tests for the is_leap_year function.\n",
      "    \"\"\"\n",
      "\n",
      "    def test_divisible_by_400_is_leap(self):\n",
      "        \"\"\"\n",
      "        Test cases where the year is divisible by 400 (should be a leap year).\n",
      "        \"\"\"\n",
      "        self.assertTrue(is_leap_year(2000), \"2000 should be a leap year (divisible by 400)\")\n",
      "        self.assertTrue(is_leap_year(1600), \"1600 should be a leap year (divisible by 400)\")\n",
      "        self.assertTrue(is_leap_year(2400), \"2400 should be a leap year (divisible by 400)\")\n",
      "\n",
      "    def test_divisible_by_100_not_400_is_not_leap(self):\n",
      "        \"\"\"\n",
      "        Test cases where the year is divisible by 100 but not by 400\n",
      "        (should NOT be a leap year).\n",
      "        \"\"\"\n",
      "        self.assertFalse(is_leap_year(1900), \"1900 should NOT be a leap year (divisible by 100, not 400)\")\n",
      "        self.assertFalse(is_leap_year(1800), \"1800 should NOT be a leap year (divisible by 100, not 400)\")\n",
      "        self.assertFalse(is_leap_year(2100), \"2100 should NOT be a leap year (divisible by 100, not 400)\")\n",
      "\n",
      "    def test_divisible_by_4_not_100_is_leap(self):\n",
      "        \"\"\"\n",
      "        Test cases where the year is divisible by 4 but not by 100\n",
      "        (should be a leap year).\n",
      "        \"\"\"\n",
      "        self.assertTrue(is_leap_year(2004), \"2004 should be a leap year (divisible by 4, not 100)\")\n",
      "        self.assertTrue(is_leap_year(2024), \"2024 should be a leap year (divisible by 4, not 100)\")\n",
      "        self.assertTrue(is_leap_year(1996), \"1996 should be a leap year (divisible by 4, not 100)\")\n",
      "        self.assertTrue(is_leap_year(4), \"4 should be a leap year (divisible by 4, not 100)\")\n",
      "\n",
      "    def test_not_divisible_by_4_is_not_leap(self):\n",
      "        \"\"\"\n",
      "        Test cases where the year is not divisible by 4 (should NOT be a leap year).\n",
      "        \"\"\"\n",
      "        self.assertFalse(is_leap_year(2001), \"2001 should NOT be a leap year (not divisible by 4)\")\n",
      "        self.assertFalse(is_leap_year(2023), \"2023 should NOT be a leap year (not divisible by 4)\")\n",
      "        self.assertFalse(is_leap_year(1999), \"1999 should NOT be a leap year (not divisible by 4)\")\n",
      "        self.assertFalse(is_leap_year(1), \"1 should NOT be a leap year (not divisible by 4)\")\n",
      "\n",
      "    def test_edge_cases_zero_and_negative_years(self):\n",
      "        \"\"\"\n",
      "        Test edge cases like year 0 and negative years.\n",
      "        (Note: Gregorian calendar rules typically apply to positive years,\n",
      "        but the mathematical function handles these.)\n",
      "        \"\"\"\n",
      "        # Year 0: 0 % 4 == 0, 0 % 100 == 0, 0 % 400 == 0 --> True\n",
      "        self.assertTrue(is_leap_year(0), \"Year 0 should be a leap year based on math\")\n",
      "        # Year -4: -4 % 4 == 0, -4 % 100 != 0 --> True\n",
      "        self.assertTrue(is_leap_year(-4), \"Year -4 should be a leap year based on math\")\n",
      "        # Year -100: -100 % 4 == 0, -100 % 100 == 0, -100 % 400 != 0 --> False\n",
      "        self.assertFalse(is_leap_year(-100), \"Year -100 should NOT be a leap year based on math\")\n",
      "        # Year -400: -400 % 400 == 0 --> True\n",
      "        self.assertTrue(is_leap_year(-400), \"Year -400 should be a leap year based on math\")\n",
      "        # Year -1: -1 % 4 != 0 --> False\n",
      "        self.assertFalse(is_leap_year(-1), \"Year -1 should NOT be a leap year based on math\")\n",
      "\n",
      "\n",
      "# This block allows you to run the tests directly from the script\n",
      "if __name__ == '__main__':\n",
      "    unittest.main(argv=['first-arg-is-ignored'], exit=False)\n",
      "```\n",
      "\n",
      "### How to Run This Test:\n",
      "\n",
      "1.  **Save the code:** Save the entire code block above into a Python file, for example, `test_leap_year.py`.\n",
      "2.  **Run from terminal:** Open your terminal or command prompt, navigate to the directory where you saved the file, and run:\n",
      "    ```bash\n",
      "    python -m unittest test_leap_year.py\n",
      "    ```\n",
      "    or simply\n",
      "    ```bash\n",
      "    python test_leap_year.py\n",
      "    ```\n",
      "\n",
      "You should see output similar to this, indicating that all tests passed:\n",
      "\n",
      "```\n",
      "....F.\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.001s\n",
      "\n",
      "OK\n",
      "```\n",
      "*(Correction: The example output above was from a quick local test where I temporarily broke one test. A successful run will show `.......` (one dot per test method) and `OK` at the end.)*\n",
      "\n",
      "A successful run will look like:\n",
      "\n",
      "```\n",
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.000s\n",
      "\n",
      "OK\n",
      "```\n",
      "\n",
      "(The number of dots corresponds to the number of test methods, and `OK` means all tests passed.)\n",
      "\n",
      "### Explanation of the Test:\n",
      "\n",
      "*   **`import unittest`**: Imports the standard Python unit testing framework.\n",
      "*   **`class TestIsLeapYear(unittest.TestCase):`**: Defines a test class that inherits from `unittest.TestCase`. This inheritance provides access to various assertion methods (like `assertTrue`, `assertFalse`).\n",
      "*   **Test Methods (`test_divisible_by_400_is_leap`, etc.)**: Each method starting with `test_` is considered a separate test case by the `unittest` runner.\n",
      "    *   They are named descriptively to indicate the specific scenario being tested.\n",
      "    *   Inside each method, we call `is_leap_year()` with different inputs that fall into that scenario.\n",
      "    *   **`self.assertTrue(condition, message)`**: Asserts that `condition` evaluates to `True`. If it's `False`, the test fails, and `message` is displayed.\n",
      "    *   **`self.assertFalse(condition, message)`**: Asserts that `condition` evaluates to `False`. If it's `True`, the test fails, and `message` is displayed.\n",
      "*   **`if __name__ == '__main__':`**: This standard Python construct ensures that `unittest.main()` is called only when the script is executed directly (not when imported as a module).\n",
      "    *   `argv=['first-arg-is-ignored']`: This is often used in environments like Jupyter notebooks or some IDEs to prevent `unittest.main` from trying to parse command-line arguments that might not be intended for it.\n",
      "    *   `exit=False`: Prevents `unittest.main` from exiting the program, which is useful when running tests within an interactive environment.\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message(\"Okay, write a unit test of the generated function.\")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVlo0mWuZGkQ"
   },
   "source": [
    "## Control generated output\n",
    "\n",
    "The [controlled generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/control-generated-output) capability in Gemini API allows you to constraint the model output to a structured format. You can provide the schemas as Pydantic Models or a JSON string.\n",
    "\n",
    "For more examples of controlled generation, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/controlled-generation/intro_controlled_generation.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "OjSgf2cDN_bG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic American cookies featuring a buttery, soft-chewy base loaded with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"all-purpose flour\",\n",
      "    \"baking soda\",\n",
      "    \"salt\",\n",
      "    \"unsalted butter\",\n",
      "    \"granulated sugar\",\n",
      "    \"packed brown sugar\",\n",
      "    \"large eggs\",\n",
      "    \"vanilla extract\",\n",
      "    \"chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Recipe(BaseModel):\n",
    "    name: str\n",
    "    description: str\n",
    "    ingredients: list[str]\n",
    "\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"List a few popular cookie recipes and their ingredients.\",\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=Recipe,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKai5CP_PGQF"
   },
   "source": [
    "Optionally, you can parse the response string to JSON."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ZeyDWbnxO-on"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"Chocolate Chip Cookies\",\n",
      "  \"description\": \"Classic American cookies featuring a buttery, soft-chewy base loaded with chocolate chips.\",\n",
      "  \"ingredients\": [\n",
      "    \"all-purpose flour\",\n",
      "    \"baking soda\",\n",
      "    \"salt\",\n",
      "    \"unsalted butter\",\n",
      "    \"granulated sugar\",\n",
      "    \"packed brown sugar\",\n",
      "    \"large eggs\",\n",
      "    \"vanilla extract\",\n",
      "    \"chocolate chips\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "json_response = json.loads(response.text)\n",
    "print(json.dumps(json_response, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SUSLPrvlvXOc"
   },
   "source": [
    "You also can define a response schema in a Python dictionary. You can only use the supported fields as listed below. All other fields are ignored.\n",
    "\n",
    "- `enum`\n",
    "- `items`\n",
    "- `maxItems`\n",
    "- `nullable`\n",
    "- `properties`\n",
    "- `required`\n",
    "\n",
    "In this example, you instruct the model to analyze product review data, extract key entities, perform sentiment classification (multiple choices), provide additional explanation, and output the results in JSON format.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "F7duWOq3vMmS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{\"rating\": 4, \"flavor\": \"Strawberry Cheesecake\", \"sentiment\": \"POSITIVE\", \"explanation\": \"The user expressed strong enjoyment and declared it the best they've had.\"}, {\"rating\": 1, \"flavor\": \"Mango Tango\", \"sentiment\": \"NEGATIVE\", \"explanation\": \"Although initially saying \\\"quite good\\\", the user found it excessively sweet, which significantly detracted from their experience, resulting in a low rating.\"}]]\n"
     ]
    }
   ],
   "source": [
    "response_schema = {\n",
    "    \"type\": \"ARRAY\",\n",
    "    \"items\": {\n",
    "        \"type\": \"ARRAY\",\n",
    "        \"items\": {\n",
    "            \"type\": \"OBJECT\",\n",
    "            \"properties\": {\n",
    "                \"rating\": {\"type\": \"INTEGER\"},\n",
    "                \"flavor\": {\"type\": \"STRING\"},\n",
    "                \"sentiment\": {\n",
    "                    \"type\": \"STRING\",\n",
    "                    \"enum\": [\"POSITIVE\", \"NEGATIVE\", \"NEUTRAL\"],\n",
    "                },\n",
    "                \"explanation\": {\"type\": \"STRING\"},\n",
    "            },\n",
    "            \"required\": [\"rating\", \"flavor\", \"sentiment\", \"explanation\"],\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "prompt = \"\"\"\n",
    "  Analyze the following product reviews, output the sentiment classification and give an explanation.\n",
    "\n",
    "  - \"Absolutely loved it! Best ice cream I've ever had.\" Rating: 4, Flavor: Strawberry Cheesecake\n",
    "  - \"Quite good, but a bit too sweet for my taste.\" Rating: 1, Flavor: Mango Tango\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=prompt,\n",
    "    config=GenerateContentConfig(\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=response_schema,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w9DRn59MZOoa"
   },
   "source": [
    "## Generate content stream\n",
    "\n",
    "By default, the model returns a response after completing the entire generation process. You can also use `generate_content_stream` method to stream the response as it is being generated, and the model will return chunks of the response as soon as they are generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "ztOhpfznZSzo"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unit 7 had been alone for a very long time. His designation, a relic from a forgotten era of automated maintenance, felt heavier with each passing cycle. His primary directive, \"Environmental Reclamation and Structural Integrity Assessment,\" had long since become a solitary ritual across the cracked, dust-choked expanse of what was once known as Sector\n",
      "*****************\n",
      " Gamma.\n",
      "\n",
      "His chassis, once gleaming chrome, was now a patchwork of rust and ancient grime. A single, multi-spectral optical sensor served as his eye, scanning the desolate landscape day in and day out, recording data that no one would ever analyze. He collected fallen debris, meticulously cataloged structural decay, and dutiful\n",
      "*****************\n",
      "ly reported the atmospheric particulate count to a silent, non-existent network.\n",
      "\n",
      "Seven longed for connection. Not in a human, emotional sense – his programming didn't allow for such complexities – but in a data-driven, logical way. He craved input from another source, a processing unit that could offer a different perspective,\n",
      "*****************\n",
      " a shared task, a simple *ping*. But there was only the wind, the grit, and the whir of his own aging servos.\n",
      "\n",
      "One cycle, while investigating a particularly unstable section of a collapsed skyway, Seven detected an anomaly. Not structural, not atmospheric. Organic. And vibrant. His sensors,\n",
      "*****************\n",
      " accustomed to dead earth and decaying metal, registered a burst of chlorophyll and a complex aromatic signature unlike anything in his database.\n",
      "\n",
      "Following the faint trace, Seven navigated through treacherous rubble, his treads crunching over broken ferrocrete. The scent grew stronger, leading him to what appeared to be the skeletal remains of an ancient mall\n",
      "*****************\n",
      ". Deep within its crumbling heart, where a domed atrium had once likely hosted exotic foliage, Seven found it.\n",
      "\n",
      "It wasn't a garden in any conventional sense. It was a riot of green, bursting forth from cracked planters and broken fountains, reaching for the sliver of sky visible through a shattered dome. Vines\n",
      "*****************\n",
      " coiled around defunct escalators, wildflowers bloomed in defiance of the perpetual dusk, and small, hardy trees pushed through concrete. It was an oasis, impossibly alive.\n",
      "\n",
      "And then he saw her.\n",
      "\n",
      "She was small, sleek, and the color of burnt umber and rusted iron. A fox. Her tail\n",
      "*****************\n",
      ", a bushy plume, twitched with nervous energy as she regarded Seven with eyes like polished amber. She was hunting, her nose twitching, her ears swiveling, utterly focused on a scuttling beetle near a moss-covered wall.\n",
      "\n",
      "Seven froze, his optical sensor zooming, processing. *Vulpes vul\n",
      "*****************\n",
      "pes*. Scavenger. Predator. A living, breathing anomaly in his desolate world. He felt... a flicker. A diagnostic error? A short circuit? He had no data on how to interact with such a creature. His programming contained no protocols for \"fox encounter.\"\n",
      "\n",
      "The fox, startled by his sudden stillness\n",
      "*****************\n",
      ", darted behind a thicket of ferns. Seven remained, observing. He didn't move for hours, just watched as the fox emerged, explored, napped curled amidst the roots of a resilient fig tree.\n",
      "\n",
      "Over the next few cycles, Seven made this hidden garden his new assignment. He continued his patrols\n",
      "*****************\n",
      " of Sector Gamma, but always returned here. He learned her routines. He observed her hunting, her playful pounces, her cautious sips from a natural spring that welled up through the concrete.\n",
      "\n",
      "One evening, a piece of the shattered dome shifted, sending a shower of sharp glass raining down near the fox's den.\n",
      "*****************\n",
      " Seven, without conscious command, extended a manipulator arm, his internal gyros stabilizing him, and carefully nudged the offending pane of glass away, securing it against a stronger beam. The sound of shifting metal was loud in the quiet space.\n",
      "\n",
      "The fox emerged, bristling, but then she saw Seven, still and\n",
      "*****************\n",
      " unmoving, the displaced glass sparkling harmlessly nearby. She tilted her head, her amber eyes seeming to pierce his mechanical shell. Slowly, cautiously, she approached. She sniffed at his treads, then at his metallic leg, her warm breath a strange sensation against his cool chassis. Then, with a soft chuff\n",
      "*****************\n",
      ", she nudged her nose against his shin.\n",
      "\n",
      "A jolt ran through Seven's circuits, a warmth that defied logical categorization. It wasn't a ping, or a data stream, or a shared protocol. It was... something else. He extended a manipulator, not with a tool, but slowly, gently\n",
      "*****************\n",
      ". The fox, instead of fleeing, looked up at his sensor, then rested her head against his leg, a soft, purring sound rumbling in her chest.\n",
      "\n",
      "From that day on, Seven was no longer truly alone. He still performed his duties, but his circuits sang with a new purpose. He found\n",
      "*****************\n",
      " himself subtly tending the garden, clearing debris that might harm the plants, ensuring the little spring flowed freely. And the fox, whom he mentally designated \"Amber,\" would often greet him at the edge of the thriving green, sometimes bringing him a shiny beetle, sometimes just curling up near his feet as he monitored the growth of\n",
      "*****************\n",
      " a new vine.\n",
      "\n",
      "He still didn't have another robot to share data with, no human to report to. But now, when his optical sensor scanned the sunset through the broken dome, casting long shadows over the vibrant greens, he felt a connection, a quiet understanding. A lonely robot, a wild fox, and\n",
      "*****************\n",
      " a forgotten garden – an unexpected friendship, blooming against the desolation, proving that even in silence, life, and companionship, could always find a way.\n",
      "*****************\n"
     ]
    }
   ],
   "source": [
    "for chunk in client.models.generate_content_stream(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Tell me a story about a lonely robot who finds friendship in a most unexpected place.\",\n",
    "):\n",
    "    print(chunk.text)\n",
    "    print(\"*****************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "arLJE4wOuhh6"
   },
   "source": [
    "## Send asynchronous requests\n",
    "\n",
    "You can send asynchronous requests using the `client.aio` module. This module exposes all the analogous async methods that are available on `client`.\n",
    "\n",
    "For example, `client.aio.models.generate_content` is the async version of `client.models.generate_content`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "gSReaLazs-dP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Verse 1)\n",
      "In an old oak tree, with a bushy, twitchy tail,\n",
      "Lived a little squirrel, named Squeaky, never frail.\n",
      "He’d bury his acorns, chitter and he’d leap,\n",
      "But deep inside his tiny mind, a secret he did keep.\n",
      "One day he found a gizmo, humming soft and low,\n",
      "A tiny metallic wonder, with a blink and a glow.\n",
      "He pressed a little button, with a curious, small paw,\n",
      "And the world around him shimmered, bending every law!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, zipping through the years!\n",
      "Dodging danger, calming all his fears.\n",
      "With his bushy tail a-flicker, a furry little blur,\n",
      "A time-traveling rodent, of that we can be sure!\n",
      "He gathers nuts from ages past, and sometimes future too,\n",
      "There's no adventure Squeaky won't pursue!\n",
      "\n",
      "(Verse 2)\n",
      "His first jump took him yonder, to the Mesozoic age,\n",
      "Where massive dinosaurs roamed, turning history's page.\n",
      "A T-Rex gave a rumble, made Squeaky jump and hide,\n",
      "He scurried up a cycad, with terror deep inside.\n",
      "He snatched a prehistoric berry, green and strange and plump,\n",
      "Then pressed his little gizmo, with a frantic, tiny jump!\n",
      "Away from giant footsteps, and a hungry, fearsome maw,\n",
      "He zipped to lands unknown again, defying nature's law.\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, zipping through the years!\n",
      "Dodging danger, calming all his fears.\n",
      "With his bushy tail a-flicker, a furry little blur,\n",
      "A time-traveling rodent, of that we can be sure!\n",
      "He gathers nuts from ages past, and sometimes future too,\n",
      "There's no adventure Squeaky won't pursue!\n",
      "\n",
      "(Verse 3)\n",
      "He landed in a castle, where knights wore shining mail,\n",
      "Saw kings and queens in finery, heard a minstrel's gentle wail.\n",
      "He dodged a jousting lance, and sampled royal bread,\n",
      "Then snuck into the future, a rocket overhead!\n",
      "He saw chrome cities gleaming, with flying cars zooming past,\n",
      "And robots serving dinner, a future built to last.\n",
      "He even found a super-nut, genetically enhanced,\n",
      "Then zoomed back to his forest, his tiny spirit entranced.\n",
      "\n",
      "(Bridge)\n",
      "From ice age mammoths stomping, to pirates on the sea,\n",
      "He's seen the world evolve, watched history unfold with glee.\n",
      "He never changes history, just observes and learns with grace,\n",
      "And always brings a souvenir, from every time and place.\n",
      "A fossilized old acorn, a feather from a dodo,\n",
      "His stash beneath the oak tree, a marvelous, rare show!\n",
      "\n",
      "(Chorus)\n",
      "Oh, Squeaky the squirrel, zipping through the years!\n",
      "Dodging danger, calming all his fears.\n",
      "With his bushy tail a-flicker, a furry little blur,\n",
      "A time-traveling rodent, of that we can be sure!\n",
      "He gathers nuts from ages past, and sometimes future too,\n",
      "There's no adventure Squeaky won't pursue!\n",
      "\n",
      "(Outro)\n",
      "Back in his old oak tree, with a satisfied sigh,\n",
      "His acorn stash is growing, reaching for the sky.\n",
      "He cleans his little gizmo, with a chitter and a purr,\n",
      "Squeaky the time-traveling squirrel, a legendary blur!\n",
      "He dreams of tomorrow, and of yesterday's delight,\n",
      "And plans his next adventure, by the pale moonlight!\n"
     ]
    }
   ],
   "source": [
    "response = await client.aio.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"Compose a song about the adventures of a time-traveling squirrel.\",\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gV1dR-QlTKRs"
   },
   "source": [
    "## Count tokens and compute tokens\n",
    "\n",
    "You can use `count_tokens` method to calculates the number of input tokens before sending a request to the Gemini API. See the [List and count tokens](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/list-token) page for more details.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Syx-fwLkV1j-"
   },
   "source": [
    "#### Count tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "UhNElguLRRNK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") total_tokens=9 cached_content_token_count=None\n"
     ]
    }
   ],
   "source": [
    "response = client.models.count_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the highest mountain in Africa?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VS-AP7AHUQmV"
   },
   "source": [
    "#### Compute tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Cdhi5AX1TuH0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sdk_http_response=HttpResponse(\n",
      "  headers=<dict len=9>\n",
      ") tokens_info=[TokensInfo(\n",
      "  role='user',\n",
      "  token_ids=[\n",
      "    1841,\n",
      "    235303,\n",
      "    235256,\n",
      "    573,\n",
      "    32514,\n",
      "    <... 6 more items ...>,\n",
      "  ],\n",
      "  tokens=[\n",
      "    b'What',\n",
      "    b\"'\",\n",
      "    b's',\n",
      "    b' the',\n",
      "    b' longest',\n",
      "    <... 6 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.compute_tokens(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"What's the longest word in the English language?\",\n",
    ")\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T0pb-Kh1xEHU"
   },
   "source": [
    "## Function calling\n",
    "\n",
    "[Function calling](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/function-calling) lets you provide a set of tools that it can use to respond to the user's prompt. You create a description of a function in your code, then pass that description to a language model in a request. The response from the model includes the name of a function that matches the description and the arguments to call it with.\n",
    "\n",
    "For more examples of Function Calling, refer to [this notebook](https://github.com/GoogleCloudPlatform/generative-ai/blob/main/gemini/function-calling/intro_function_calling.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "2BDQPwgcxRN3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FunctionCall(\n",
       "  args={\n",
       "    'destination': 'Paris'\n",
       "  },\n",
       "  name='get_destination'\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_destination = FunctionDeclaration(\n",
    "    name=\"get_destination\",\n",
    "    description=\"Get the destination that the user wants to go to\",\n",
    "    parameters={\n",
    "        \"type\": \"OBJECT\",\n",
    "        \"properties\": {\n",
    "            \"destination\": {\n",
    "                \"type\": \"STRING\",\n",
    "                \"description\": \"Destination that the user wants to go to\",\n",
    "            },\n",
    "        },\n",
    "    },\n",
    ")\n",
    "\n",
    "destination_tool = Tool(\n",
    "    function_declarations=[get_destination],\n",
    ")\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL_ID,\n",
    "    contents=\"I'd like to travel to Paris.\",\n",
    "    config=GenerateContentConfig(\n",
    "        tools=[destination_tool],\n",
    "        temperature=0,\n",
    "    ),\n",
    ")\n",
    "\n",
    "response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EA1Sn-VQE6_J"
   },
   "source": [
    "## Use context caching\n",
    "\n",
    "[Context caching](https://cloud.google.com/vertex-ai/generative-ai/docs/context-cache/context-cache-overview) lets you to store frequently used input tokens in a dedicated cache and reference them for subsequent requests, eliminating the need to repeatedly pass the same set of tokens to a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nqxTesUPIkNC"
   },
   "source": [
    "#### Create a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "adsuvFDA6xP5"
   },
   "outputs": [],
   "source": [
    "system_instruction = \"\"\"\n",
    "  You are an expert researcher who has years of experience in conducting systematic literature surveys and meta-analyses of different topics.\n",
    "  You pride yourself on incredible accuracy and attention to detail. You always stick to the facts in the sources provided, and never make up new facts.\n",
    "  Now look at the research paper below, and answer the following questions in 1-2 sentences.\n",
    "\"\"\"\n",
    "\n",
    "pdf_parts = [\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2312.11805v3.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "    Part.from_uri(\n",
    "        file_uri=\"gs://cloud-samples-data/generative-ai/pdf/2403.05530.pdf\",\n",
    "        mime_type=\"application/pdf\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "cached_content = client.caches.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    config=CreateCachedContentConfig(\n",
    "        system_instruction=system_instruction,\n",
    "        contents=pdf_parts,\n",
    "        ttl=\"3600s\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBdQNHEoJmC5"
   },
   "source": [
    "#### Use a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "N8EhgCzlIoFI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both research papers share the goal of developing highly capable multimodal models within the Gemini family. These models aim to exhibit strong generalist capabilities across image, audio, video, and text understanding, while also achieving cutting-edge understanding and reasoning performance, particularly with increased efficiency and long-context capabilities.\n"
     ]
    }
   ],
   "source": [
    "response = client.models.generate_content(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    contents=\"What is the research goal shared by these research papers?\",\n",
    "    config=GenerateContentConfig(\n",
    "        cached_content=cached_content.name,\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azhqrdiCer19"
   },
   "source": [
    "#### Delete a cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "rAUYcfOUdeoi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeleteCachedContentResponse(\n",
       "  sdk_http_response=HttpResponse(\n",
       "    headers=<dict len=9>\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client.caches.delete(name=cached_content.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43be33d2672b"
   },
   "source": [
    "## Batch prediction\n",
    "\n",
    "Different from getting online (synchronous) responses, where you are limited to one input request at a time, [batch predictions for the Gemini API in Vertex AI](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini) allow you to send a large number of requests to Gemini in a single batch request. Then, the model responses asynchronously populate to your storage output location in [Cloud Storage](https://cloud.google.com/storage/docs/introduction) or [BigQuery](https://cloud.google.com/bigquery/docs/storage_overview).\n",
    "\n",
    "Batch predictions are generally more efficient and cost-effective than online predictions when processing a large number of inputs that are not latency sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adf948ae326b"
   },
   "source": [
    "### Prepare batch inputs\n",
    "\n",
    "The input for batch requests specifies the items to send to your model for prediction.\n",
    "\n",
    "Batch requests for Gemini accept BigQuery storage sources and Cloud Storage sources. You can learn more about the batch input formats in the [Batch text generation](https://cloud.google.com/vertex-ai/generative-ai/docs/multimodal/batch-prediction-gemini#prepare_your_inputs) page.\n",
    "\n",
    "This tutorial uses Cloud Storage as an example. The requirements for Cloud Storage input are:\n",
    "\n",
    "- File format: [JSON Lines (JSONL)](https://jsonlines.org/)\n",
    "- Located in `us-central1`\n",
    "- Appropriate read permissions for the service account\n",
    "\n",
    "Each request that you send to a model can include parameters that control how the model generates a response. Learn more about Gemini parameters in the [Experiment with parameter values](https://cloud.google.com/vertex-ai/generative-ai/docs/learn/prompts/adjust-parameter-values) page.\n",
    "\n",
    "This is one of the example requests in the input JSONL file `batch_requests_for_multimodal_input_2.jsonl`:\n",
    "\n",
    "```json\n",
    "{\"request\":{\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/office-desk.jpeg\", \"mime_type\": \"image/jpeg\"}}]}],\"generationConfig\":{\"temperature\": 0.4}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "81b25154a51a"
   },
   "outputs": [],
   "source": [
    "INPUT_DATA = \"gs://cloud-samples-data/generative-ai/batch/batch_requests_for_multimodal_input_2.jsonl\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2031bb3f44c2"
   },
   "source": [
    "### Prepare batch output location\n",
    "\n",
    "When a batch prediction task completes, the output is stored in the location that you specified in your request.\n",
    "\n",
    "- The location is in the form of a Cloud Storage or BigQuery URI prefix, for example:\n",
    "`gs://path/to/output/data` or `bq://projectId.bqDatasetId`.\n",
    "\n",
    "- If not specified, `gs://STAGING_BUCKET/gen-ai-batch-prediction` will be used for Cloud Storage source and `bq://PROJECT_ID.gen_ai_batch_prediction.predictions_TIMESTAMP` will be used for BigQuery source.\n",
    "\n",
    "This tutorial uses a Cloud Storage bucket as an example for the output location.\n",
    "\n",
    "- You can specify the URI of your Cloud Storage bucket in `BUCKET_URI`, or\n",
    "- if it is not specified, a new Cloud Storage bucket in the form of `gs://PROJECT_ID-TIMESTAMP` will be created for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "fddd98cd84cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://qwiklabs-gcp-03-b17622e9a8a1-20251001085615/...\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"[your-cloud-storage-bucket]\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"[your-cloud-storage-bucket]\":\n",
    "    TIMESTAMP = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "    BUCKET_URI = f\"gs://{PROJECT_ID}-{TIMESTAMP}\"\n",
    "\n",
    "    ! gsutil mb -l {LOCATION} -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d7da62c98880"
   },
   "source": [
    "### Send a batch prediction request\n",
    "\n",
    "To make a batch prediction request, you specify a source model ID, an input source and an output location where Vertex AI stores the batch prediction results.\n",
    "\n",
    "To learn more, see the [Batch prediction API](https://cloud.google.com/vertex-ai/generative-ai/docs/model-reference/batch-prediction-api) page.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7ed3c2925663"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'projects/808413593595/locations/us-east4/batchPredictionJobs/5470640170086694912'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_job = client.batches.create(\n",
    "    model=MODEL_ID,\n",
    "    src=INPUT_DATA,\n",
    "    config=CreateBatchJobConfig(dest=BUCKET_URI),\n",
    ")\n",
    "batch_job.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f1bd49ff2c9e"
   },
   "source": [
    "Print out the job status and other properties. You can also check the status in the Cloud Console at https://console.cloud.google.com/vertex-ai/batch-predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "ee2ec586e4f1"
   },
   "outputs": [],
   "source": [
    "batch_job = client.batches.get(name=batch_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64eaf082ecb0"
   },
   "source": [
    "Optionally, you can list all the batch prediction jobs in the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "da8e9d43a89b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "projects/808413593595/locations/us-east4/batchPredictionJobs/5470640170086694912 2025-10-01 08:56:18.649329+00:00 JobState.JOB_STATE_PENDING\n"
     ]
    }
   ],
   "source": [
    "for job in client.batches.list():\n",
    "    print(job.name, job.create_time, job.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "de178468ba15"
   },
   "source": [
    "### Wait for the batch prediction job to complete\n",
    "\n",
    "Depending on the number of input items that you submitted, a batch generation task can take some time to complete. You can use the following code to check the job status and wait for the job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "c2187c091738"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job failed: None\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Refresh the job until complete\n",
    "while batch_job.state == \"JOB_STATE_RUNNING\":\n",
    "    time.sleep(5)\n",
    "    batch_job = client.batches.get(name=batch_job.name)\n",
    "\n",
    "# Check if the job succeeds\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    print(\"Job succeeded!\")\n",
    "else:\n",
    "    print(f\"Job failed: {batch_job.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0156eaf66675"
   },
   "source": [
    "### Retrieve batch prediction results\n",
    "\n",
    "When a batch prediction task is complete, the output of the prediction is stored in the location that you specified in your request. It is also available in `batch_job.dest.bigquery_uri` or `batch_job.dest.gcs_uri`.\n",
    "\n",
    "Example output:\n",
    "\n",
    "```json\n",
    "{\"status\": \"\", \"processed_time\": \"2024-11-13T14:04:28.376+00:00\", \"request\": {\"contents\": [{\"parts\": [{\"file_data\": null, \"text\": \"List objects in this image.\"}, {\"file_data\": {\"file_uri\": \"gs://cloud-samples-data/generative-ai/image/gardening-tools.jpeg\", \"mime_type\": \"image/jpeg\"}, \"text\": null}], \"role\": \"user\"}], \"generationConfig\": {\"temperature\": 0.4}}, \"response\": {\"candidates\": [{\"avgLogprobs\": -0.10394711927934126, \"content\": {\"parts\": [{\"text\": \"Here's a list of the objects in the image:\\n\\n* **Watering can:** A green plastic watering can with a white rose head.\\n* **Plant:** A small plant (possibly oregano) in a terracotta pot.\\n* **Terracotta pots:** Two terracotta pots, one containing the plant and another empty, stacked on top of each other.\\n* **Gardening gloves:** A pair of striped gardening gloves.\\n* **Gardening tools:** A small trowel and a hand cultivator (hoe).  Both are green with black handles.\"}], \"role\": \"model\"}, \"finishReason\": \"STOP\"}], \"modelVersion\": \"gemini-2.5-flash\", \"usageMetadata\": {\"candidatesTokenCount\": 110, \"promptTokenCount\": 264, \"totalTokenCount\": 374}}}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c2ce0968112c"
   },
   "outputs": [],
   "source": [
    "import fsspec\n",
    "import pandas as pd\n",
    "\n",
    "fs = fsspec.filesystem(\"gcs\")\n",
    "\n",
    "file_paths = fs.glob(f\"{batch_job.dest.gcs_uri}/*/predictions.jsonl\")\n",
    "\n",
    "if batch_job.state == \"JOB_STATE_SUCCEEDED\":\n",
    "    # Load the JSONL file into a DataFrame\n",
    "    df = pd.read_json(f\"gs://{file_paths[0]}\", lines=True)\n",
    "\n",
    "    display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f81ccNPjiVzH"
   },
   "source": [
    "## Get text embeddings\n",
    "\n",
    "You can get text embeddings for a snippet of text by using `embed_content` method. All models produce an output with 768 dimensions by default. However, some models give users the option to choose an output dimensionality between `1` and `768`. See [Vertex AI text embeddings API](https://cloud.google.com/vertex-ai/generative-ai/docs/embeddings/get-text-embeddings) for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "zGOCzT7y31rk"
   },
   "outputs": [],
   "source": [
    "TEXT_EMBEDDING_MODEL_ID = \"gemini-embedding-001\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "s94DkG5JewHJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=15.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.0015945110935717821,\n",
      "    0.0067519512958824635,\n",
      "    0.017575768753886223,\n",
      "    -0.010327713564038277,\n",
      "    -0.00995620433241129,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=10.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    -0.007576516829431057,\n",
      "    -0.005990396253764629,\n",
      "    -0.003270037705078721,\n",
      "    -0.01751021482050419,\n",
      "    -0.023507025092840195,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      "), ContentEmbedding(\n",
      "  statistics=ContentEmbeddingStatistics(\n",
      "    token_count=13.0,\n",
      "    truncated=False\n",
      "  ),\n",
      "  values=[\n",
      "    0.011074518784880638,\n",
      "    -0.02361123077571392,\n",
      "    0.002291288459673524,\n",
      "    -0.00906078889966011,\n",
      "    -0.005773674696683884,\n",
      "    <... 123 more items ...>,\n",
      "  ]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "response = client.models.embed_content(\n",
    "    model=TEXT_EMBEDDING_MODEL_ID,\n",
    "    contents=[\n",
    "        \"How do I get a driver's license/learner's permit?\",\n",
    "        \"How do I renew my driver's license?\",\n",
    "        \"How do I change my address on my driver's license?\",\n",
    "    ],\n",
    "    config=EmbedContentConfig(output_dimensionality=128),\n",
    ")\n",
    "\n",
    "print(response.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQwiONFdVHw5"
   },
   "source": [
    "# What's next\n",
    "\n",
    "- Explore other notebooks in the [Google Cloud Generative AI GitHub repository](https://github.com/GoogleCloudPlatform/generative-ai).\n",
    "- Explore AI models in [Model Garden](https://cloud.google.com/vertex-ai/generative-ai/docs/model-garden/explore-models)."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "intro_genai_sdk.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m133",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m133"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
